\section{Data Files}

The search engines are general and scalable and could therefore work with any database that is consistent. The database of choice for this project is a large dataset consisting of a snapshot of Wikipedia from 2010, which has been converted into a simple text format. Subsets of this dataset has been used to measure the complexity of the search engines. These subsets are prefixes of the entire dataset of increasing size: 0.1, 1, 2, 5, 10, 20, 50, 100, 200, 400, and 800 MB, with the size of entire dataset being 5.7 GB. The increasing size of these files will be helpful in illustrating the complexities of the search engines implemented. 

Whenever one of these files are referred to, they are done so by the size of the file only, for example "the 5 MB file". 

\section{Source code for the project}

Every implementation and test used for this project can be found in the following Github repository: 

\begin{center}
\begin{tcolorbox}[width=\textwidth, colframe={lightgray}, arc=0mm, colbacktitle=gray, coltitle=black] 
\begin{center}
        \href{https://github.com/jona605a/searchengine}{https://github.com/jona605a/searchengine}
\end{center}
\end{tcolorbox}
\end{center}

The repository contains a \texttt{README.md} file that explains how to run the code in the Basic part and Advanced part. 
