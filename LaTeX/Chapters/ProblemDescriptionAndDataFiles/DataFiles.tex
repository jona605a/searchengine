\section{Data Files}

The search engines are general and scalable and could therefore work with any database that is consistent. The database of choice for this project is a large dataset consisting of a snapshot of Wikipedia from 2010, which has been converted into a simple text format. Subsets of this dataset has been used to measure the complexity of the search engines. These subsets are prefixes of the entire dataset of increasing size: 0.1, 1, 2, 5, 10, 20, 50, 100, 200, 400, and 800 MB, with the size of entire dataset being 1.7 GB. It contains 990,248,478 words


The size of the file WestburyLab.wikicorp.201004.txt is 1.7GB. It contains 990,248,478 words in over 2 million documents. The files WestburyLab.wikicorp.201004X.txt are prefixes of the original file of size X. All files are compressed with bzip2. 



