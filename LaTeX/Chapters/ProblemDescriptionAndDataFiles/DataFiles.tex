\section{Data Files}

The search engines are general and scalable and could therefore work with any database that is consistent. The database of choice for this project is a large dataset consisting of a snapshot of Wikipedia from 2010, which has been converted into a simple text format. Subsets of this dataset has been used to measure the complexity of the search engines. These subsets are prefixes of the entire dataset of increasing size: 0.1, 1, 2, 5, 10, 20, 50, 100, 200, 400, and 800 MB, with the size of entire dataset being 5.7 GB. 

Whenever one of these files are referred to, they are done so by the size of the file only, for example "the 5 MB file". 
