\section{Full text Indexes}\label{sec:index10}

With these many string matching algorithms ready to use, it is now time to introduce Index10.0, Index10.1, Index11.0 and Index11.1. These indices all support queries of the form "Which documents contain the entire sentence 'X Y Z'?". 

The difference between the two Index10s and Index11s is how they select which documents to search through. The Index11s use more memory in the index which results in a better heuristic for which documents contain a true occurrence of the query. 

\subsection{Index 10.0 and 10.1}
The indexing of Index10.0 and 10.1 is exactly like in Index8: A hash map where each unique word has a key, and the value of each key is an Article list, representing which articles the word is present in. The indexing time is therefore still linear. However, the search functions differ, as these indexes support full-text indexing.

Since the index only contains information about individual words and their article lists, in order to search for a whole sentence, it is necessary to combine the lists to determine which articles to search through. The immediate answer is to use the intersection of the lists, as any article in which the sentence 'X Y Z' appears must contain the words X, Y and Z individually. This is precisely what Index10.0 and 10.1 do. 

The problem of finding the intersection of the article lists in which a collection of words appear may seem familiar. This corresponds exactly to using Boolean search on all the words in the query sentence where all words are AND'ed together. And while this seems like an excellent opportunity to use the boolean method again by constructing a boolean syntax tree, the actual implementation uses a more straightforward way of finding the intersection between the lists. 

When searching for a query sentence containing $q$ words, the article list for each word in the query is looked up and their intersection is calculated. Looking up the lists take $O(q)$ time and finding their intersection takes $O(q\cdot a)$ time. All the articles in this intersection are then opened as a file and searched through to find the exact match of the query using KMP in index 10.0 and Boyer-Moore in 10.1. 

Using the string matching algorithms, there are two ways of matching a pattern with a text. Either return a boolean value of whether the pattern matches somewhere in the text or return all the occurrences of the pattern in the text. Both methods have been implemented, but the aim of these indices was to only answer the question "Which documents contain entire sentence 'X Y Z'?" and not where. This means that the algorithms are slightly modified to return once they find the first match. This also improves the complexity for Boyer-Moore, as explained that section, to have a worst-case complexity of $O(m)$, like KMP. 

If the number of articles in the intersection is $\alpha$ and the average article length is $n$, the total complexity of the search is $O(\alpha\cdot n)$, which has a worst-case when $\alpha=a$ and it is linear in the size of the entire document, like Index1-2. This, however, is highly dependent on the query itself and how common the words in the query are. The index uses the same structure and space as in Index8. 

\subsection{Index 11.0 and 11.1}
Having considered the problem of many articles in the article intersection, Index11.0 and 11.1 try to gain a better heuristic for when a sentence appears in an article. This is done by creating an index that hashes both a word and the next two words in the sentence. This is defined as a triple

\begin{itemize}
    \item[] \textbf{Definition} A triple of a sentence is three consecutive words in the sentence. \\There are thereby $w-2$ triples in a sentence with $w\geq2$ words.
\end{itemize}

When searching for a query, The article list for each triple of the query is looked up and the intersection of the article list is calculated. These indexes thereby used $O(tri_t\cdot a)$ space, where a is the number of articles and $tri_t$ is the number of unique triples in a text. Technically, t is upper bounded by $O(u^3)$ (where u is upper bounded by $O(n)$) as there could be all possible configurations of triples. This poor space complexity should especially be considered if the nature of a search text is prone to contain all possible combinations of triples of unique words (eg if DNA basepairs were considered as words and the full text were a long DNA string).  $O(u^3)$ is here considered to be an overestimation when considering an English text. It is therefore that the variable $tri_t$ is introduced.

The search function in Index 11.0 simply returns the intersection as the result. This result may be incorrect as it is possible that all triples of a query exist in a text but not in one continuous sequence. This search function is however very fast as it simply needs to look up $tri_q$ articles list, where $tri_q$ is the number of triples in the query, and find the intersection of these queries. This takes $O(tri_q\cdot a)$ time as looking op $tri_q$ articles list in the hash table takes $O(tri_q)$ time and performing $tri_q - 1 $ AND operations to calculate the intersection takes $O(tri_q\cdot a)$ time.

The search function in index 11.1 searches through all the articles that contain all of the triples of the query, using Boyer-Moore. This gives an exact result and is predicted to be much faster than index 10, as many articles may be excluded, making the total search string much smaller. The factor of the reduction of the search string depends tremendously on the content of the query and the search text.


\subsection{Methods for benchmarking}
To compare the indexing time, the code that indexed the different full-text search engines where timed using criterion. All full-text search engines were indexed over different file sizes.

To compare the Full-text search function against each other 1000 full-text queries of length 5 and 1000 full-text queries of length 25  were found for each file size. Each query where generated by choosing one of the articles, with $\frac{1}{a}$ chance, and then finding a randomly placed word sequence of length 5 or 25. 

For each file size, searching for the 1000 queries of length 5 and the 1000 queries of length 25 were timed using Criterion.





