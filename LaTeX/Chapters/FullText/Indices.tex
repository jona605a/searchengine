\section{Full text Indices}\label{sec:index10}

With these many string matching algorithms ready to use, it is now time to introduce Index10.0, Index10.1, Index10.2, Index11.0 and Index11.1. These indices all support queries of the form "Which documents contain the entire sentence 'X Y Z'?". 

The difference between the Index10s and Index11s is how they select which documents to search through. The Index11s use more memory in the index which results in a better heuristic for which documents contain a true occurrence of the query. 

\subsection{Index10.0, 10.1 and 10.2}
The indexing of Index10.0, 10.1 and 10.2 (collectively Index10) is exactly like in Index8: A hash map where each unique word has a key, and the value of each key is an Article list, representing which articles the word is present in. The indexing time is therefore still linear. However, the search functions differ, as these indices support full-text indexing. Because the search functions need to go back to an article and search through it, Index10 also gathers each individual article in the input file into individual files, so that they can more easily be accessed. 

Since the index only contains information about individual words and their article lists, in order to search for a whole sentence, it is necessary to combine the lists to determine which articles to search through. The immediate answer is to use the intersection of the lists, as any article in which the sentence 'X Y Z' appears must contain the words X, Y and Z individually. This is precisely what Index10 do. 

The problem of finding the intersection of the article lists in which a collection of words appear may seem familiar. This corresponds exactly to using Boolean search on all the words in the query sentence where all words are AND'ed together. And while this seems like an excellent opportunity to use the Boolean method again by constructing a Boolean query tree, the actual implementation uses a more straightforward way of finding the intersection between the lists. 

When searching for a query sentence containing $q$ words, the article list for each word in the query is looked up and their intersection is calculated. Looking up the lists take $O(q)$ time and finding their intersection takes $O(q\cdot a)$ time. All the articles in this intersection are then opened as a file and searched through to find the exact match of the query using KMP in Index10.0, Boyer-Moore in 10.1 and Apostolico-Giancarlo in 10.2. Having to open any article, the search function uses $O(t)$ space, where $t$ is the length of the longest article. 

Using the string matching algorithms, there are two ways of matching a pattern with a text. Either return a Boolean value of whether the pattern matches somewhere in the text or return the position of all the occurrences of the pattern in the text. Both methods have been implemented, but the aim of these indices was to only answer the question "Which documents contain the entire sentence 'X Y Z'?" and not where. This means that the algorithms are slightly modified to return once they find the first match. This also improves the complexity for the regular Boyer-Moore, as explained in that section, to have a worst-case complexity of $O(m)$, like KMP. 

If the number of articles in the intersection is $\alpha$ and the average article length is $n$, the total complexity of the search is $O(q\cdot a+\alpha\cdot n)$, which has a worst-case when $\alpha=a$ and it is linear in the size of the entire input file, like Index1 and 2. This, however, is highly dependent on the query itself and how common the words in the query are. 

\subsection{Index 11.0 and 11.1}
Having considered the problem of many articles in the article intersection, Index11.0 and 11.1 try to gain a better heuristic for when a sentence appears in an article. This is done by creating an index that hashes both a word and the next two words in the sentence and records the article list in which these three appear. This is defined as a triple:

\begin{itemize}
    \item[] \textbf{Definition} A triple of a sentence is three consecutive words in the sentence. \\There are thereby $w-2$ triples in a sentence with $w\geq2$ words.
\end{itemize}

When searching for a query, the article list for each triple of the query is looked up and the intersection of the article list is calculated, similar to Index10. These indices thereby use space linear to the number of unique triples in a text, potentially $n-2$. Note that the number of triples in a text is also bounded by $O(u^3)$ as there can not be more triples than all possible combinations of every unique word. Being bounded by both, the number of unique triples is thereby $O(\min(n,u^3))$ and the space complexity becomes $O(\min(n,u^3)\cdot a)$. 

Index11.0 implements a form of \textit{Fuzzy search}, where the articles in its output are not guaranteed to actually contain the query. The fuzzy search is thereby an attempt at returning a likely estimate without having to use any string matching algorithms that use linear time in every article. 

The search function in Index 11.0 simply returns the article intersection of all triples in the query as the result. It is possible to return a false positive if all the triples in the search query individually occur in the article, but not in a continuous sentence. False negatives are however impossible since any article in which the search query appears must contain each triple in the query. This search function is very fast as it simply needs to look up $O(q)$ articles lists, where $q$ is the length of the query, and find the intersection of these queries. This takes $O(q\cdot a)$ time as looking op $q$ articles list in the hash table takes $O(q)$ time and performing $O(q)$ AND operations to calculate the intersection takes $O(q\cdot a)$ time.

Index11.1 uses string matching like Index10 but only on the articles in the triples intersection. The string matching is using the regular Boyer-Moore algorithm and returns upon finding the first occurrence of the query in the text. This gives an exact result and is expected to be much faster than Index10, as many articles may be excluded, making the total search text smaller. The factor of the reduction of the search string depends highly on the content of the query and the search text. The time complexity of the search function in Index11.1, therefore, becomes $O(q\cdot a + \alpha_{tri}\cdot n)$, where $\alpha_{tri}$ is the number of articles that are in the triples intersection. 

\subsection{Methods for benchmarking}
To compare the indexing time, the code that indexed the different full-text search engines were timed using criterion. All full-text search engines were indexed over different file sizes, repeated only 10 times instead of the usual 100.

To compare the Full-text search function against each other 1000 full-text queries of length 5 and 1000 full-text queries of length 25  were found for each file size. Each query was generated by choosing one of the articles uniformly at random and then finding a randomly placed word sequence of length 5 or 25. 

For each file size, searching for the 1000 queries of length 5 and the 1000 queries of length 25 were timed using Criterion.
